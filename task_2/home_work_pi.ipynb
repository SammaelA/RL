{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fbFNnO8u7Ly1"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week02_value_based/mdp.py\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2F0B7FtY7LzP"
      },
      "source": [
        "# HW Part 1: Value iteration convergence\n",
        "\n",
        "### Find an MDP for which value iteration takes long to converge  (1 pts)\n",
        "\n",
        "When we ran value iteration on the small frozen lake problem, the last iteration where an action changed was iteration 6--i.e., value iteration computed the optimal policy at iteration 6. Are there any guarantees regarding how many iterations it'll take value iteration to compute the optimal policy? There are no such guarantees without additional assumptions--we can construct the MDP in such a way that the greedy policy will change after arbitrarily many iterations.\n",
        "\n",
        "Your task: define an MDP with at most 3 states and 2 actions, such that when you run value iteration, the optimal action changes at iteration >= 50. Use discount=0.95. (However, note that the discount doesn't matter here--you can construct an appropriate MDP with any discount.)\n",
        "\n",
        "Note: value function must change at least once after iteration >=50, not necessarily change on every iteration till >=50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AzFvKIoI7LzQ"
      },
      "outputs": [],
      "source": [
        "#found by brute-force a few cells below\n",
        "p = [[0.0020946662477109167, 0.9954791609153069, 0.0024261728369821627], [0.4538413348620296, 0.004065787387917773, 0.5420928777500525], [0.522748024241617, 0.41834838748690134, 0.05890358827148161], [0.05590032913797978, 0.0017678255299463784, 0.9423318453320739], [0.12609838942288928, 0.17942788229668682, 0.6944737282804239], [0.9839084384646596, 0.015045046357288542, 0.0010465151780519042], [0.03483519001535749, 0.8119503721452164, 0.1532144378394262], [0.41088645989570227, 2.2527786594240957e-07, 0.5891133148264318], [0.7828775489485023, 0.023787120201481934, 0.19333533085001595], [0.137988056778625, 0.47657301770172766, 0.3854389255196474], [0.11761899526259705, 0.622869417924548, 0.25951158681285497], [0.6254538403214218, 0.01979950825775079, 0.3547466514208273], [0.02030840960043218, 0.7963690496829573, 0.1833225407166105], [0.9621013640962319, 0.0013187183567453036, 0.036579917547022694], [0.05177988885745235, 0.5488312270059246, 0.39938888413662305], [0.8476710608067988, 5.824102369925579e-06, 0.1523231150908313]]\n",
        "\n",
        "transition_probs = {\n",
        "      's0': {\n",
        "          'a0': {'s0': p[0][0], 's1': p[0][1], 's2': p[0][2]},\n",
        "          'a1': {'s0': p[1][0], 's1': p[1][1], 's2': p[1][2]}\n",
        "      },\n",
        "      's1': {\n",
        "          'a0': {'s0': p[2][0], 's1': p[2][1], 's2': p[2][2]},\n",
        "          'a1': {'s0': p[3][0], 's1': p[3][1], 's2': p[3][2]}\n",
        "      },\n",
        "      's2': {\n",
        "          'a0': {'s0': p[4][0], 's1': p[4][1], 's2': p[4][2]},\n",
        "          'a1': {'s0': p[5][0], 's1': p[5][1], 's2': p[5][2]}\n",
        "      }\n",
        "  }\n",
        "rewards = {\n",
        "      's0': {\n",
        "        'a0': {'s0': p[6][0], 's1': p[6][1], 's2': p[6][2]},\n",
        "        'a1': {'s0': p[7][0], 's1': p[7][1], 's2': p[7][2]},\n",
        "        'a2': {'s0': p[8][0], 's1': p[8][1], 's2': p[8][2]}\n",
        "        },\n",
        "      's1': {\n",
        "        'a0': {'s0': p[9][0], 's1': p[9][1], 's2': p[9][2]},\n",
        "        'a1': {'s0': p[10][0], 's1': p[10][1], 's2': p[10][2]},\n",
        "        'a2': {'s0': p[11][0], 's1': p[11][1], 's2': p[11][2]}\n",
        "        },\n",
        "      's2': {\n",
        "        'a0': {'s0': p[12][0], 's1': p[12][1], 's2': p[12][2]},\n",
        "        'a1': {'s0': p[13][0], 's1': p[13][1], 's2': p[13][2]},\n",
        "        'a2': {'s0': p[14][0], 's1': p[14][1], 's2': p[14][2]}\n",
        "        }\n",
        "  }\n",
        "\n",
        "from mdp import MDP\n",
        "from mdp import FrozenLakeEnv\n",
        "from numpy import random\n",
        "import numpy as np \n",
        "mdp = MDP(transition_probs, rewards, initial_state=random.choice(tuple(transition_probs.keys())))\n",
        "# Feel free to change the initial_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "#these functions were used in a cell below, but not declared in code\n",
        "#I used templates for these functions taken from similar task from here:\n",
        "#https://github.com/yandexdataschool/Practical_RL/blob/56df1f7612d5cb46b8343b7a95c6ae69900b0880/week02_value_based/seminar_vi.ipynb\n",
        "#It works, I suppose\n",
        "\n",
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
        "    next_states = mdp.get_next_states(state, action)\n",
        "    arr = []\n",
        "    for next_state, prob in next_states.items():\n",
        "        arr.append(prob * (mdp.get_reward(state, action, next_state) + gamma*state_values[next_state]))\n",
        "    return np.sum(arr)\n",
        "\n",
        "\n",
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Computes next V(s) .Please do not change state_values in process. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "    arr = []\n",
        "    for a in mdp.get_possible_actions(state):\n",
        "        arr.append(get_action_value(mdp, state_values, state, a, gamma))\n",
        "    return np.max(arr)\n",
        "\n",
        "\n",
        "def get_optimal_action(mdp, state_values, state, gamma=0.9):\n",
        "    \"\"\" Finds optimal action. \"\"\"\n",
        "    if mdp.is_terminal(state):\n",
        "        return None\n",
        "    map = {}\n",
        "    for a in mdp.get_possible_actions(state):\n",
        "        map[a] = get_action_value(mdp, state_values, state, a, gamma)\n",
        "    return max(map, key=lambda key: map[key])\n",
        "\n",
        "def value_iteration(mdp, state_values=None, gamma=0.9, num_iter=1000, min_difference=1e-5, verbose = False):\n",
        "    \"\"\" performs num_iter value iteration steps starting from state_values. Same as before but in a function \"\"\"\n",
        "    zeros = {s: 0 for s in mdp.get_all_states()}\n",
        "    state_values = state_values or zeros\n",
        "    for i in range(num_iter):\n",
        "        # Compute new state values using the functions you defined above. It must be a dict {state : new_V(state)}\n",
        "        new_state_values = {}\n",
        "        for s in state_values:\n",
        "            new_state_values[s] = get_new_state_value(mdp, state_values, s, gamma)\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Compute difference\n",
        "        diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
        "        if verbose:\n",
        "            print(\"iter %4i   |   diff: %6.5f   |   V(start): %.3f \" % (i, diff, new_state_values[mdp._initial_state]))\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            break\n",
        "\n",
        "    return state_values, i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "1AjmOke77LzQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after iteration 0\n",
            "N actions changed = 2 \n",
            "\n",
            "after iteration 2\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 3\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 5\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 6\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 8\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 9\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 11\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 12\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 14\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 15\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 17\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 18\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 20\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 21\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 23\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 24\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 26\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 27\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 29\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 30\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 32\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 33\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 34\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 36\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 37\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 39\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 40\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 42\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 43\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 45\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 46\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 48\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 49\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 51\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 52\n",
            "N actions changed = 1 \n",
            "\n",
            "after iteration 54\n",
            "N actions changed = 1 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "gamma = 0.95\n",
        "\n",
        "state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
        "                   for state in sorted(mdp.get_all_states())])\n",
        "\n",
        "for i in range(100):\n",
        "    state_values, _ = value_iteration(mdp, state_values, num_iter=1)\n",
        "\n",
        "    new_policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
        "                           for state in sorted(mdp.get_all_states())])\n",
        "\n",
        "    n_changes = (policy != new_policy).sum()\n",
        "    if (n_changes > 0):\n",
        "        print(\"after iteration %i\" % i)\n",
        "        print(\"N actions changed = %i \\n\" % n_changes)\n",
        "    policy = new_policy\n",
        "\n",
        "# please ignore iter 0 at each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "new best res  4\n",
            "new best res  11\n",
            "new best res  13\n",
            "new best res  19\n",
            "new best res  22\n",
            "new best res  28\n",
            "new best res  30\n",
            "new best res  54\n",
            "The best res  54  with params  [[0.0020946662477109167, 0.9954791609153069, 0.0024261728369821627], [0.4538413348620296, 0.004065787387917773, 0.5420928777500525], [0.522748024241617, 0.41834838748690134, 0.05890358827148161], [0.05590032913797978, 0.0017678255299463784, 0.9423318453320739], [0.12609838942288928, 0.17942788229668682, 0.6944737282804239], [0.9839084384646596, 0.015045046357288542, 0.0010465151780519042], [0.03483519001535749, 0.8119503721452164, 0.1532144378394262], [0.41088645989570227, 2.2527786594240957e-07, 0.5891133148264318], [0.7828775489485023, 0.023787120201481934, 0.19333533085001595], [0.137988056778625, 0.47657301770172766, 0.3854389255196474], [0.11761899526259705, 0.622869417924548, 0.25951158681285497], [0.6254538403214218, 0.01979950825775079, 0.3547466514208273], [0.02030840960043218, 0.7963690496829573, 0.1833225407166105], [0.9621013640962319, 0.0013187183567453036, 0.036579917547022694], [0.05177988885745235, 0.5488312270059246, 0.39938888413662305], [0.8476710608067988, 5.824102369925579e-06, 0.1523231150908313]]\n"
          ]
        }
      ],
      "source": [
        "#brute force to find the required MDP\n",
        "p = [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1],\n",
        "     [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1],\n",
        "     [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
        "best_params = p\n",
        "best_res = 0\n",
        "max_iters = 100000\n",
        "for _ in range(max_iters):\n",
        "  for p1 in p:\n",
        "    p1[0] = np.random.uniform(0, 1) ** 3\n",
        "    p1[1] = np.random.uniform(0, 1) ** 4\n",
        "    p1[2] = np.random.uniform(0, 1) ** 3\n",
        "    s = p1[0] + p1[1] + p1[2]\n",
        "    p1[0] /= s\n",
        "    p1[1] /= s\n",
        "    p1[2] /= s\n",
        "\n",
        "  transition_probs = {\n",
        "      's0': {\n",
        "          'a0': {'s0': p[0][0], 's1': p[0][1], 's2': p[0][2]},\n",
        "          'a1': {'s0': p[1][0], 's1': p[1][1], 's2': p[1][2]}\n",
        "      },\n",
        "      's1': {\n",
        "          'a0': {'s0': p[2][0], 's1': p[2][1], 's2': p[2][2]},\n",
        "          'a1': {'s0': p[3][0], 's1': p[3][1], 's2': p[3][2]}\n",
        "      },\n",
        "      's2': {\n",
        "          'a0': {'s0': p[4][0], 's1': p[4][1], 's2': p[4][2]},\n",
        "          'a1': {'s0': p[5][0], 's1': p[5][1], 's2': p[5][2]}\n",
        "      }\n",
        "  }\n",
        "  rewards = {\n",
        "      's0': {\n",
        "        'a0': {'s0': p[6][0], 's1': p[6][1], 's2': p[6][2]},\n",
        "        'a1': {'s0': p[7][0], 's1': p[7][1], 's2': p[7][2]},\n",
        "        'a2': {'s0': p[8][0], 's1': p[8][1], 's2': p[8][2]}\n",
        "        },\n",
        "      's1': {\n",
        "        'a0': {'s0': p[9][0], 's1': p[9][1], 's2': p[9][2]},\n",
        "        'a1': {'s0': p[10][0], 's1': p[10][1], 's2': p[10][2]},\n",
        "        'a2': {'s0': p[11][0], 's1': p[11][1], 's2': p[11][2]}\n",
        "        },\n",
        "      's2': {\n",
        "        'a0': {'s0': p[12][0], 's1': p[12][1], 's2': p[12][2]},\n",
        "        'a1': {'s0': p[13][0], 's1': p[13][1], 's2': p[13][2]},\n",
        "        'a2': {'s0': p[14][0], 's1': p[14][1], 's2': p[14][2]}\n",
        "        }\n",
        "  }\n",
        "  mdp = MDP(transition_probs, rewards, initial_state=random.choice(tuple(transition_probs.keys())))\n",
        "  state_values = {s: 0 for s in mdp.get_all_states()}\n",
        "  policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
        "                    for state in sorted(mdp.get_all_states())])\n",
        "\n",
        "  br = 0\n",
        "  for i in range(100):\n",
        "      state_values, _ = value_iteration(mdp, state_values, num_iter=1)\n",
        "\n",
        "      new_policy = np.array([get_optimal_action(mdp, state_values, state, gamma)\n",
        "                            for state in sorted(mdp.get_all_states())])\n",
        "\n",
        "      n_changes = (policy != new_policy).sum()\n",
        "      if (n_changes > 0):\n",
        "          br = i\n",
        "          #print(\"after iteration %i\" % i)\n",
        "          #print(\"N actions changed = %i \\n\" % n_changes)\n",
        "      policy = new_policy\n",
        "  if br > best_res:\n",
        "    best_params = p\n",
        "    best_res = br\n",
        "    print(\"new best res \", br)\n",
        "  if (best_res > 50):\n",
        "    break\n",
        "print(\"The best res \", best_res, \" with params \", best_params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jk3J-NM97LzQ"
      },
      "source": [
        "### Value iteration convervence proof (1 pts)\n",
        "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
        "\n",
        "Update of value function in value iteration can be rewritten in a form of Bellman operator:\n",
        "\n",
        "$$(TV)(s) = \\max_{a \\in \\mathcal{A}}\\mathbb{E}\\left[ r_{k+1} + \\gamma V(s_{k+1}) | s_t = s, a_t = a\\right]$$\n",
        "\n",
        "Value iteration algorithm with Bellman operator:\n",
        "\n",
        "---\n",
        "&nbsp;&nbsp; Initialize $V_0$\n",
        "\n",
        "&nbsp;&nbsp; **for** $k = 0,1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $V_{k+1} \\leftarrow TV_k$\n",
        "\n",
        "&nbsp;&nbsp;**end for**\n",
        "\n",
        "---\n",
        "\n",
        "In [lecture](https://docs.google.com/presentation/d/1lz2oIUTvd2MHWKEQSH8hquS66oe4MZ_eRvVViZs2uuE/edit#slide=id.g4fd6bae29e_2_4) we established contraction property of bellman operator:\n",
        "\n",
        "$$\n",
        "||TV - TU||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
        "$$\n",
        "\n",
        "For all $V, U$\n",
        "\n",
        "Using contraction property of Bellman operator, Banach fixed-point theorem and Bellman equations prove that value function converges to $V^*$ in value iterateion$\n",
        "\n",
        "Proof:\n",
        "$V^*$ is a fixed point of operator $T$, then $TV^* = V^*$\n",
        "\n",
        "Consider $||V_{k+1} - V^*||_{\\infty}$ =  $||TV_{k} - TV^*||_{\\infty}$ $\\le$ $\\gamma ||V_{k} - V^*||_{\\infty}$ $\\le$ $\\gamma^{k+1} ||V_{0} - V^*||_{\\infty}$\n",
        "\n",
        "As $||U - V||_{\\infty} \\ge 0$, $||V_{0} - V^*||_{\\infty}$ is finite, and $ 0 < \\gamma < 1$, it means $\\lim\\limits_{k \\to +\\infty} ||V_{k} - V^*|| = 0$\n",
        "\n",
        "Q.E.D."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iXyZNNbD7LzQ"
      },
      "source": [
        "### Asynchronious value iteration (2 pts)\n",
        "\n",
        "Consider the following algorithm:\n",
        "\n",
        "---\n",
        "\n",
        "Initialize $V_0$\n",
        "\n",
        "**for** $k = 0,1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Select some state $s_k \\in \\mathcal{S}$    \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; $V(s_k) := (TV)(s_k)$\n",
        "\n",
        "**end for**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Note that unlike common value iteration, here we update only a single state at a time.\n",
        "\n",
        "**Homework.** Prove the following proposition:\n",
        "\n",
        "If for all $s \\in \\mathcal{S}$, $s$ appears in the sequence $(s_0, s_1, ...)$ infinitely often, then $V$ converges to $V*$\n",
        "\n",
        "Define $D_{i}^{k} = |V_{k}(s_i) - V^*(s_i)|$\n",
        "Let's divide $\\mathcal{S}$ into subsequences with the same states. All of them would be infinite.\n",
        "\n",
        "$||V_{k+1}-V^*||_{\\infty} \\le ||V_{k+1}-V^*||_1 = \\sum\\limits_{i=1}^{|\\mathcal{S}|} D_i^{k+1}$\n",
        "\n",
        "Taking a limit on both sides:\n",
        "$0 \\le \\lim\\limits_{k \\to +\\infty} ||V_{k+1}-V^*||_{\\infty} \\le \\lim\\limits_{k \\to +\\infty}\\sum\\limits_{i=1}^{|\\mathcal{S}|} D_i^{k+1}$ (1)\n",
        "\n",
        "For each i $\\lim\\limits_{k \\to +\\infty} D_i^{k+1} = \\lim\\limits_{k \\to +\\infty} |V_{k+1}(s_i) - V^*(s_i)|$\n",
        "\n",
        "Define $C_i(k) = \\sum\\limits_{j=1}^{k} \\mathcal{I}(S[j] = s_i)$, $\\mathcal{I}$ is an indicator function. $C_i(k) \\to +\\infty$ when $k \\to +\\infty$.\n",
        "\n",
        "In this terms $\\lim\\limits_{k \\to +\\infty} D_i^{k+1} = \\lim\\limits_{k \\to +\\infty} |V_{k+1}(s_i) - V^*(s_i)| \\le \\lim\\limits_{k \\to +\\infty} \\gamma^{C_i(k)} |V_0(s_i) - V^*(s_i)| = 0$\n",
        "\n",
        "So we can swap sum and limit operations\n",
        "$\\lim\\limits_{k \\to +\\infty}\\sum\\limits_{i=1}^{|\\mathcal{S}|} D_i^{k+1} = \\sum\\limits_{i=1}^{|\\mathcal{S}|}\\lim\\limits_{k \\to +\\infty} D_i^{k+1}$\n",
        "\n",
        "Put this into (1) and get $\\lim\\limits_{k \\to +\\infty} ||V_{k+1}-V^*||_{\\infty} = 0$\n",
        "\n",
        "Q.E.D.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrB0-2f7LzR"
      },
      "source": [
        "# HW Part 2: Policy iteration\n",
        "\n",
        "## Policy iteration implementateion (3 pts)\n",
        "\n",
        "Let's implement exact policy iteration (PI), which has the following pseudocode:\n",
        "\n",
        "---\n",
        "Initialize $\\pi_0$   `// random or fixed action`\n",
        "\n",
        "For $n=0, 1, 2, \\dots$\n",
        "- Compute the state-value function $V^{\\pi_{n}}$\n",
        "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
        "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
        "---\n",
        "\n",
        "Unlike VI, policy iteration has to maintain a policy - chosen actions from all states - and estimate $V^{\\pi_{n}}$ based on this policy. It only changes policy once values converged.\n",
        "\n",
        "\n",
        "Below are a few helpers that you may or may not use in your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "9zg0_u3U7LzR"
      },
      "outputs": [],
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's1': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UnrmkDd67LzR"
      },
      "source": [
        "Let's write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
        "\n",
        "Unlike VI, this time you must find the exact solution, not just a single iteration.\n",
        "\n",
        "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
        "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
        "\n",
        "You'll have to solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "E2QK8eqA7LzS"
      },
      "outputs": [],
      "source": [
        "def compute_vpi(mdp, policy, gamma):\n",
        "    \"\"\"\n",
        "    Computes V^pi(s) FOR ALL STATES under given policy.\n",
        "    :param policy: a dict of currently chosen actions {s : a}\n",
        "    :returns: a dict {state : V^pi(state) for all states}\n",
        "    \"\"\"\n",
        "    all_states = mdp.get_all_states()\n",
        "    N = len(all_states)\n",
        "    states_dict = dict(zip(all_states, np.arange(N)))\n",
        "    system = np.diag(np.ones(N))\n",
        "    A = np.zeros(N)\n",
        "    for i, state in enumerate(all_states):\n",
        "        if policy[state]:\n",
        "            for next_state in mdp.get_next_states(state, policy[state]):\n",
        "                prob = mdp.get_transition_prob(state, policy[state], next_state)\n",
        "                reward = mdp.get_reward(state, policy[state], next_state)\n",
        "                A[i] += prob * reward\n",
        "                system[i][states_dict[next_state]] -= gamma * prob\n",
        "            \n",
        "    vpi = np.linalg.solve(system, A).tolist()\n",
        "    return dict(zip(all_states, vpi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "uEHiEqbY7LzS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': -1.2155511811023618, 's1': -0.6545275590551177, 's2': -1.3435039370078736}\n"
          ]
        }
      ],
      "source": [
        "test_policy = {s: np.random.choice(\n",
        "    mdp.get_possible_actions(s)) for s in mdp.get_all_states()}\n",
        "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
        "\n",
        "print(new_vpi)\n",
        "\n",
        "assert type(\n",
        "    new_vpi) is dict, \"compute_vpi must return a dict {state : V^pi(state) for all states}\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LCWaPo117LzT"
      },
      "source": [
        "Once we've got new state values, it's time to update our policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "wajw1FeW7LzU"
      },
      "outputs": [],
      "source": [
        "def compute_qpi(mdp, vpi, gamma, state, action):\n",
        "    arr = []\n",
        "    for next_state in mdp.get_next_states(state, action):\n",
        "        v = mdp.get_transition_prob(state, action, next_state)* (mdp.get_reward(state, action, next_state) + gamma * vpi[next_state])\n",
        "        arr.append(v)\n",
        "    return np.sum(arr)\n",
        "\n",
        "def compute_new_policy(mdp, vpi, gamma):\n",
        "    \"\"\"\n",
        "    Computes new policy as argmax of state values\n",
        "    :param vpi: a dict {state : V^pi(state) for all states}\n",
        "    :returns: a dict {state : optimal action for all states}\n",
        "    \"\"\"\n",
        "    \n",
        "    policy = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        actions = mdp.get_possible_actions(state)\n",
        "        if actions:\n",
        "            actions_qpi = [compute_qpi(mdp, vpi, gamma, state, action) for action in actions]\n",
        "            best_action_id = np.argmax(actions_qpi)\n",
        "            policy[state] = actions[best_action_id]\n",
        "        else:\n",
        "            policy[state] = ''\n",
        "            \n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "hLTExIUz7LzU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n"
          ]
        }
      ],
      "source": [
        "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
        "\n",
        "print(new_policy)\n",
        "\n",
        "assert type(\n",
        "    new_policy) is dict, \"compute_new_policy must return a dict {state : optimal action for all states}\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o_tC7U6r7LzV"
      },
      "source": [
        "__Main loop__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tes1scqn7LzV"
      },
      "outputs": [],
      "source": [
        "def random_policy(mdp):\n",
        "    policy = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        actions = mdp.get_possible_actions(state)\n",
        "        if actions:\n",
        "            policy[state] = actions[np.random.randint(0, len(actions))]\n",
        "        else:\n",
        "            policy[state] = ''\n",
        "    return policy\n",
        "\n",
        "def vpi_diff(vpi, new_vpi):\n",
        "    return np.linalg.norm(np.array(list(new_vpi.values())) - np.array(list(vpi.values())), ord=np.inf)\n",
        "\n",
        "def policy_iteration(mdp, policy=None, gamma=0.9, num_iter=1000, min_difference=1e-5):\n",
        "    \"\"\" \n",
        "    Run the policy iteration loop for num_iter iterations or till difference between V(s) is below min_difference.\n",
        "    If policy is not given, initialize it at random.\n",
        "    \"\"\"\n",
        "    if policy is None:\n",
        "        policy = random_policy(mdp)\n",
        "    state_values = compute_vpi(mdp, policy, gamma)\n",
        "    for i in range(num_iter):\n",
        "        new_policy = compute_new_policy(mdp, state_values, gamma)\n",
        "        new_state_values = compute_vpi(mdp, new_policy, gamma)\n",
        "        diff = vpi_diff(state_values, new_state_values)\n",
        "        policy = new_policy\n",
        "        state_values = new_state_values\n",
        "        if diff <= min_difference:\n",
        "            break\n",
        "\n",
        "    return state_values, policy, i"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h-ooQ7vj7LzV"
      },
      "source": [
        "__Your PI Results__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Swkv4o9W7LzW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frozen lake\n",
            "Average VI reward  0.6496000000000001\n",
            "Average iteration count: 21.0\n",
            "-----------------------\n",
            "Average PI reward  0.6593600000000001\n",
            "Average iteration count: 4.28\n",
            "-----------\n",
            "Large frozen lake\n",
            "Average VI reward  0.7409599999999998\n",
            "Average iteration count: 33.0\n",
            "-----------------------\n",
            "Average PI reward  0.74472\n",
            "Average iteration count: 7.36\n",
            "-----------\n"
          ]
        }
      ],
      "source": [
        "def value_iteration_test(mdp, gamma=0.9, iters = 500):\n",
        "    state_values, total_iters = value_iteration(mdp, gamma=gamma)\n",
        "\n",
        "    reward_sums = []\n",
        "    for _ in range(iters):\n",
        "        s = mdp.reset()\n",
        "        rewards = []\n",
        "        for t in range(100):\n",
        "            optimal_action = get_optimal_action(mdp, state_values, s, gamma)\n",
        "            s, r, done, _ = mdp.step(optimal_action)\n",
        "            rewards.append(r)\n",
        "            if done:\n",
        "                break\n",
        "        reward_sums.append(np.sum(rewards))\n",
        "    \n",
        "    return np.mean(reward_sums), total_iters\n",
        "\n",
        "def policy_iteration_test(mdp, gamma=0.9, iters = 500):\n",
        "    vpi, policy, total_iters = policy_iteration(mdp, gamma=gamma)\n",
        "    \n",
        "    reward_sums = []\n",
        "    for _ in range(iters):\n",
        "        s = mdp.reset()\n",
        "        rewards = []\n",
        "        for t in range(100):\n",
        "            optimal_action = policy[s]\n",
        "            s, r, done, _ = mdp.step(optimal_action)\n",
        "            rewards.append(r)\n",
        "            if done:\n",
        "                break\n",
        "        reward_sums.append(np.sum(rewards))\n",
        "    \n",
        "    return np.mean(reward_sums), total_iters\n",
        "\n",
        "def vi_pi_compare(mdp, gamma, iters, tries):\n",
        "    \n",
        "    vi_iters = []\n",
        "    pi_iters = []\n",
        "    \n",
        "    vi_rewards = []\n",
        "    pi_rewards = []\n",
        "    \n",
        "    for _ in range(tries):\n",
        "        r, it = value_iteration_test(mdp, gamma, iters)\n",
        "        vi_rewards.append(r)\n",
        "        vi_iters.append(it)\n",
        "\n",
        "        r, it = policy_iteration_test(mdp, gamma, iters)\n",
        "        pi_rewards.append(r)\n",
        "        pi_iters.append(it)\n",
        "\n",
        "    print(\"Average VI reward \", np.mean(vi_rewards))\n",
        "    print(\"Average iteration count:\", np.mean(vi_iters))\n",
        "    print(\"-----------------------\")\n",
        "    print(\"Average PI reward \", np.mean(pi_rewards))\n",
        "    print(\"Average iteration count:\", np.mean(pi_iters))\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "mdp = FrozenLakeEnv(slip_chance=0.2)\n",
        "print(\"Frozen lake\")\n",
        "vi_pi_compare(mdp, gamma, 500, 25)\n",
        "print(\"-----------\")\n",
        "\n",
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "print(\"Large frozen lake\")\n",
        "vi_pi_compare(mdp, gamma, 500, 25)\n",
        "print(\"-----------\")\n",
        "\n",
        "#policy iteration gives slitly higher reward and much less iteration needed to reach it"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r_LvZz0i7LzW"
      },
      "source": [
        "## Policy iteration convergence (3 pts)\n",
        "\n",
        "**Note:** Assume that $\\mathcal{S}, \\mathcal{A}$ are finite.\n",
        "\n",
        "We can define another Bellman operator:\n",
        "\n",
        "$$(T_{\\pi}V)(s) = \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[r + \\gamma V(s')\\right]$$\n",
        "\n",
        "And rewrite policy iteration algorithm in operator form:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Initialize $\\pi_0$\n",
        "\n",
        "**for** $k = 0,1,2,...$ **do**\n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Solve $V_k = T_{\\pi_k}V_k$   \n",
        "\n",
        "&nbsp;&nbsp;&nbsp;&nbsp; Select $\\pi_{k+1}$ s.t. $T_{\\pi_{k+1}}V_k = TV_k$\n",
        "\n",
        "**end for**\n",
        "\n",
        "---\n",
        "\n",
        "To prove convergence of the algorithm we need to prove two properties: contraction an monotonicity.\n",
        "\n",
        "#### Monotonicity (0.5 pts)\n",
        "\n",
        "For all $V, U$ if $V(s) \\le U(s)$   $\\forall s \\in \\mathcal{S}$ then $(T_\\pi V)(s) \\le (T_\\pi U)(s)$   $\\forall s \\in  \\mathcal{S}$\n",
        "\n",
        "Proof:\n",
        "\n",
        "\n",
        "$T_\\pi V(s) - T_\\pi U(s) = \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[r + \\gamma V(s')\\right] - \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[r + \\gamma U(s')\\right] = $\n",
        "\n",
        "$= \\mathbb{E}_{r, s'|s, a = \\pi(s)}\\left[\\gamma(V(s') - U(s'))\\right]$ = {As $\\mathcal{S}$ is finite, write expectation as a sum} = \n",
        "\n",
        "$ \\sum\\limits_{s'} \\underbrace{P(r, s' | s, a = \\pi(s))}_{\\ge 0}\\underbrace{\\gamma}_{> 0}\\underbrace{(V(s') - U(s'))}_{\\le 0} \\le 0$\n",
        "\n",
        "Q.E.D.\n",
        "\n",
        "#### Contraction (1 pts)\n",
        "\n",
        "$$\n",
        "||T_\\pi V - T_\\pi U||_{\\infty} \\le \\gamma ||V - U||_{\\infty}\n",
        "$$\n",
        "\n",
        "For all $V, U$\n",
        "\n",
        "Proof:\n",
        "\n",
        "$||T_\\pi V - T_\\pi U||_{\\infty} = \\max\\limits_s \\left|\\sum\\limits_{s'} P(r, s' | s, a = \\pi(s))\\gamma(V(s') - U(s'))\\right| \\le $\n",
        "\n",
        "$\\le \\max\\limits_s \\sum\\limits_{s'} \\gamma P(r, s' | s, a = \\pi(s)) |V(s') - U(s')| \\le $\n",
        "\n",
        "$\\le \\max\\limits_s \\sum\\limits_{s'} \\gamma\\max\\limits_{s'}|V(s') - U(s')|P(r, s' | s, a = \\pi(s)) = $\n",
        "\n",
        "$= \\gamma||V - U||_{\\infty}\\max\\limits_s \\sum\\limits_{s'} P(r, s' | s, a = \\pi(s)) = $ {sum of probabilities for all states $s'$ equals to 1} = \n",
        "\n",
        "$= \\gamma||V - U||_{\\infty}$\n",
        "\n",
        "Q.E.D.\n",
        "\n",
        "#### Convergence (1.5 pts)\n",
        "\n",
        "Prove that there exists iteration $k_0$ such that $\\pi_k = \\pi^*$ for all $k \\ge k_0$\n",
        "\n",
        "Proof:\n",
        "\n",
        "We know about monotonicity of $V_k : V_k \\le V_{k+1} \\le V^*$ and using monotonivcity of $TV$ we get $T_{\\pi_{k+1}} V_k \\le T_{\\pi_{k+1}}V_{k+1} = V_{k+1}$\n",
        "\n",
        "Consider $|V_{k+1}-V^*| = V^* - V_{k+1} \\le V^* - T_{\\pi_{k+1}}V_k = T_{\\pi_{k+1}}V^* - T_{\\pi_{k+1}}V_k = |T_{\\pi_{k+1}}V^* - T_{\\pi_{k+1}}V_k|$\n",
        "\n",
        "Then $||V_{k+1}-V^*||_{\\infty} \\le ||T_{\\pi_{k+1}}V^* - T_{\\pi_{k+1}}V_k||_{\\infty} \\le$ {contraction} $\\le \\gamma||V_k-V^*||_{\\infty} \\le \\gamma^k||V_0-V^*||_{\\infty}$\n",
        "\n",
        "It means tnat $V_k$ monotonously converges to $V^*$. Hovewer, we have only a finite set of available policies $\\Pi$ (as it's determined by actions and states, both finite) and all $V_k$ are produces by one of the policies from $\\Pi$. Assume $\\alpha = \\min\\limits_{\\pi \\in \\Pi : ||V(\\pi)-V^*||_{\\infty} > 0}||V(\\pi)-V^*||_{\\infty}$.\n",
        "\n",
        "Due to convergence $V_k : \\exists k_0 : \\forall k > k_0 \\implies ||V_{k}-V^*||_{\\infty} \\le 0.5*\\alpha$, but it means $\\forall k > k_0 \\implies ||V_{k}-V^*||_{\\infty} = 0 $ due to definition of $\\alpha$ and $\\pi_k = \\pi^*$ for all $k \\ge k_0$\n",
        "\n",
        "Q.E.D."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
