{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqZ2EwnTZdC8"
      },
      "source": [
        "# Deep Q-Network implementation.\n",
        "\n",
        "This homework shamelessly demands you to implement DQN — an approximate Q-learning algorithm with experience replay and target networks — and see if it works any better this way.\n",
        "\n",
        "Original paper:\n",
        "https://arxiv.org/pdf/1312.5602.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv7XJfXaZdC9"
      },
      "source": [
        "**This notebook is given for debug.** The main task is in the other notebook (**homework_pytorch_main**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
        "\n",
        "**We suggest the following pipeline:** First implement debug notebook then implement the main one.\n",
        "\n",
        "**About evaluation:** All points are given for the main notebook with one exception: if agent fails to beat the threshold in main notebook you can get 1 pt (instead of 3 pts) for beating the threshold in debug notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ioIEVODJZdC9"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/atari_wrappers.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/utils.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/replay_buffer.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week04_approx_rl/framebuffer.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u8OFQOtGojc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages (from gymnasium) (1.26.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages (from gymnasium) (4.7.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDZqlI3kZdC9"
      },
      "source": [
        "__Frameworks__ - we'll accept this homework in any deep learning framework. This particular notebook was designed for PyTorch, but you find it easy to adapt it to almost any Python-based deep learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dsYq558wZdC-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6ypPZ8e6ZdC-"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j8EGNlSZdC-"
      },
      "source": [
        "### CartPole again\n",
        "\n",
        "Another env can be used without any modification of the code. State space should be a single vector, actions should be discrete.\n",
        "\n",
        "CartPole is the simplest one. It should take several minutes to solve it.\n",
        "\n",
        "For LunarLander it can take 1-2 hours to get 200 points (a good score) on Colab and training progress does not look informative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v-5u-CcQZdC-"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CartPole-v1\"\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    # some envs are wrapped with a time limit wrapper by default\n",
        "    env = gym.make(ENV_NAME, render_mode=\"rgb_array\").unwrapped\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AmFXRrkqZdC-"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = make_env()\n",
        "env.reset()\n",
        "plt.imshow(env.render())\n",
        "state_shape, n_actions = env.observation_space.shape, env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOyWgOmvZdC-"
      },
      "source": [
        "### Building a network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqpThLZXZdC-"
      },
      "source": [
        "We now need to build a neural network that can map observations to state q-values.\n",
        "The model does not have to be huge yet. 1-2 hidden layers with < 200 neurons and ReLU activation will probably be enough. Batch normalization and dropout can spoil everything here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UVlpkvZOZdC-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sammael/RL_2023/task_1/.conda/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# those who have a GPU but feel unfair to use it can uncomment:\n",
        "# device = torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFva1cpyZdC-"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(nn.Module):\n",
        "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = n_actions\n",
        "        self.state_shape = state_shape\n",
        "        # Define your network body here. Please make sure agent is fully contained here\n",
        "        assert len(state_shape) == 1\n",
        "        state_dim = state_shape[0]\n",
        "        <YOUR CODE>\n",
        "\n",
        "\n",
        "    def forward(self, state_t):\n",
        "        \"\"\"\n",
        "        takes agent's observation (tensor), returns qvalues (tensor)\n",
        "        :param state_t: a batch states, shape = [batch_size, *state_dim=4]\n",
        "        \"\"\"\n",
        "        # Use your network to compute qvalues for given state\n",
        "        qvalues = <YOUR CODE>\n",
        "\n",
        "        assert qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
        "        assert (\n",
        "            len(qvalues.shape) == 2 and\n",
        "            qvalues.shape[0] == state_t.shape[0] and\n",
        "            qvalues.shape[1] == n_actions\n",
        "        )\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def get_qvalues(self, states):\n",
        "        \"\"\"\n",
        "        like forward, but works on numpy arrays, not tensors\n",
        "        \"\"\"\n",
        "        model_device = next(self.parameters()).device\n",
        "        states = torch.tensor(states, device=model_device, dtype=torch.float32)\n",
        "        qvalues = self.forward(states)\n",
        "        return qvalues.data.cpu().numpy()\n",
        "\n",
        "    def sample_actions(self, qvalues):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "\n",
        "        should_explore = np.random.choice(\n",
        "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
        "        return np.where(should_explore, random_actions, best_actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1s5JKzZdC-"
      },
      "outputs": [],
      "source": [
        "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vazC0DPQZdC_"
      },
      "source": [
        "Now let's try out our agent to see if it raises any errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-Sg1cqPZdC_"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000, seed=None):\n",
        "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(n_games):\n",
        "        s, _ = env.reset(seed=seed)\n",
        "        reward = 0\n",
        "        for _ in range(t_max):\n",
        "            qvalues = agent.get_qvalues([s])\n",
        "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
        "            s, r, terminated, truncated, _ = env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        rewards.append(reward)\n",
        "    return np.mean(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0NzjUEZdC_"
      },
      "source": [
        "### Experience replay\n",
        "For this assignment, we provide you with experience replay buffer. If you implemented experience replay buffer in last week's assignment, you can copy-paste it here in main notebook **to get 2 bonus points**.\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHyCO4TuZdC_"
      },
      "source": [
        "#### The interface is fairly simple:\n",
        "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
        "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
        "* `len(exp_replay)` - returns number of elements stored in replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQEHwR1AZdC_"
      },
      "outputs": [],
      "source": [
        "from replay_buffer import ReplayBuffer\n",
        "exp_replay = ReplayBuffer(10)\n",
        "\n",
        "for _ in range(30):\n",
        "    exp_replay.add(env.reset()[0], env.action_space.sample(), 1.0, env.reset()[0], done=False)\n",
        "\n",
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(5)\n",
        "\n",
        "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RnFX5sfZdC_"
      },
      "outputs": [],
      "source": [
        "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
        "    \"\"\"\n",
        "    Play the game for exactly n_steps, record every (s,a,r,s', done) to replay buffer.\n",
        "    Whenever game ends due to termination or truncation, add record with done=terminated and reset the game.\n",
        "    It is guaranteed that env has terminated=False when passed to this function.\n",
        "\n",
        "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
        "\n",
        "    :returns: return sum of rewards over time and the state in which the env stays\n",
        "    \"\"\"\n",
        "    s = initial_state\n",
        "    sum_rewards = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return sum_rewards, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXXmFEKGZdC_",
        "outputId": "d1b66847-a141-4406-9697-7ebd194fdb6a"
      },
      "outputs": [],
      "source": [
        "# testing your code.\n",
        "exp_replay = ReplayBuffer(2000)\n",
        "\n",
        "state, _ = env.reset()\n",
        "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
        "\n",
        "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
        "# just make sure you know what your code does\n",
        "assert len(exp_replay) == 1000, \\\n",
        "    \"play_and_record should have added exactly 1000 steps, \" \\\n",
        "    \"but instead added %i\" % len(exp_replay)\n",
        "is_dones = list(zip(*exp_replay._storage))[-1]\n",
        "\n",
        "assert 0 < np.mean(is_dones) < 0.1, \\\n",
        "    \"Please make sure you restart the game whenever it is 'done' and \" \\\n",
        "    \"record the is_done correctly into the buffer. Got %f is_done rate over \" \\\n",
        "    \"%i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
        "        np.mean(is_dones), len(exp_replay))\n",
        "\n",
        "for _ in range(100):\n",
        "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
        "    assert act_batch.shape == (10,), \\\n",
        "        \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
        "    assert reward_batch.shape == (10,), \\\n",
        "        \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
        "    assert is_done_batch.shape == (10,), \\\n",
        "        \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
        "    assert [int(i) in (0, 1) for i in is_dones], \\\n",
        "        \"is_done should be strictly True or False\"\n",
        "    assert [0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
        "\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoVGsnHRZdC_"
      },
      "source": [
        "### Target networks\n",
        "\n",
        "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
        "\n",
        "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
        "\n",
        "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
        "\n",
        "![img](https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BLJCNiuZdC_",
        "outputId": "6181261a-60cf-4626-fbe6-930a6ccd9896"
      },
      "outputs": [],
      "source": [
        "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
        "# This is how you can load weights from agent into target network\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_GGShX3ZdC_"
      },
      "source": [
        "### Learning with... Q-learning\n",
        "Here we write a function similar to `agent.update` from tabular q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hbg-xANZdC_"
      },
      "source": [
        "Compute Q-learning TD error:\n",
        "\n",
        "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
        "\n",
        "With Q-reference defined as\n",
        "\n",
        "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot max_{a'} Q_{target}(s', a') $$\n",
        "\n",
        "Where\n",
        "* $Q_{target}(s',a')$ denotes Q-value of next state and next action predicted by __target_network__\n",
        "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
        "* $\\gamma$ is a discount factor defined two cells above.\n",
        "\n",
        "\n",
        "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
        "\n",
        "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxrEOC7mZdC_"
      },
      "outputs": [],
      "source": [
        "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
        "                    agent, target_network,\n",
        "                    gamma=0.99,\n",
        "                    check_shapes=False,\n",
        "                    device=device):\n",
        "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
        "    states = torch.tensor(states, device=device, dtype=torch.float32)    # shape: [batch_size, *state_shape]\n",
        "    actions = torch.tensor(actions, device=device, dtype=torch.int64)    # shape: [batch_size]\n",
        "    rewards = torch.tensor(rewards, device=device, dtype=torch.float32)  # shape: [batch_size]\n",
        "    # shape: [batch_size, *state_shape]\n",
        "    next_states = torch.tensor(next_states, device=device, dtype=torch.float)\n",
        "    is_done = torch.tensor(\n",
        "        is_done.astype('float32'),\n",
        "        device=device,\n",
        "        dtype=torch.float32,\n",
        "    )  # shape: [batch_size]\n",
        "    is_not_done = 1 - is_done\n",
        "\n",
        "    # get q-values for all actions in current states\n",
        "    predicted_qvalues = agent(states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # compute q-values for all actions in next states\n",
        "    predicted_next_qvalues = target_network(next_states)  # shape: [batch_size, n_actions]\n",
        "\n",
        "    # select q-values for chosen actions\n",
        "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]  # shape: [batch_size]\n",
        "\n",
        "    # compute V*(next_states) using predicted next q-values\n",
        "    next_state_values = <YOUR CODE>\n",
        "\n",
        "    assert next_state_values.dim() == 1 and next_state_values.shape[0] == states.shape[0], \\\n",
        "        \"must predict one value per state\"\n",
        "\n",
        "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
        "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
        "    # you can multiply next state values by is_not_done to achieve this.\n",
        "    target_qvalues_for_actions = <YOUR CODE>\n",
        "\n",
        "    # mean squared error loss to minimize\n",
        "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
        "\n",
        "    if check_shapes:\n",
        "        assert predicted_next_qvalues.data.dim() == 2, \\\n",
        "            \"make sure you predicted q-values for all actions in next state\"\n",
        "        assert next_state_values.data.dim() == 1, \\\n",
        "            \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
        "        assert target_qvalues_for_actions.data.dim() == 1, \\\n",
        "            \"there's something wrong with target q-values, they must be a vector\"\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgZKcPPnZdC_"
      },
      "source": [
        "Sanity checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp8eREoDZdC_"
      },
      "outputs": [],
      "source": [
        "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(10)\n",
        "\n",
        "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
        "                       agent, target_network,\n",
        "                       gamma=0.99, check_shapes=True)\n",
        "loss.backward()\n",
        "\n",
        "assert loss.requires_grad and tuple(loss.data.size()) == (), \\\n",
        "    \"you must return scalar loss - mean over batch\"\n",
        "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() != 0), \\\n",
        "    \"loss must be differentiable w.r.t. network weights\"\n",
        "assert np.all(next(target_network.parameters()).grad is None), \\\n",
        "    \"target network should not have grads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A1QtGVqZdC_"
      },
      "source": [
        "### Main loop\n",
        "\n",
        "It's time to put everything together and see if it learns anything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lAUT94JZdC_"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOk81bdZZdC_",
        "outputId": "2fd2404e-19e5-4ebe-b0e1-c6f7593db790"
      },
      "outputs": [],
      "source": [
        "seed = <YOUR CODE: your favourite random seed>\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13K5t2CTZdDA",
        "outputId": "031d4a0a-99c3-4cc3-f7c3-77e4d8a5a331"
      },
      "outputs": [],
      "source": [
        "state_dim = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "state, _ = env.reset(seed=seed)\n",
        "\n",
        "agent = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network = DQNAgent(state_dim, n_actions, epsilon=1).to(device)\n",
        "target_network.load_state_dict(agent.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD7PAlwQZdDA",
        "outputId": "aeb4bb67-4776-4b02-e558-d9d1a3306d47"
      },
      "outputs": [],
      "source": [
        "REPLAY_BUFFER_SIZE = 10**4\n",
        "\n",
        "exp_replay = ReplayBuffer(REPLAY_BUFFER_SIZE)\n",
        "for i in range(100):\n",
        "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
        "        print(\"\"\"\n",
        "            Less than 100 Mb RAM available.\n",
        "            Make sure the buffer size in not too huge.\n",
        "            Also check, maybe other processes consume RAM heavily.\n",
        "            \"\"\"\n",
        "             )\n",
        "        break\n",
        "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
        "    if len(exp_replay) == REPLAY_BUFFER_SIZE:\n",
        "        break\n",
        "print(len(exp_replay))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl2VCEYQZdDA"
      },
      "outputs": [],
      "source": [
        "# # for something more complicated than CartPole\n",
        "\n",
        "# timesteps_per_epoch = 1\n",
        "# batch_size = 32\n",
        "# total_steps = 3 * 10**6\n",
        "# decay_steps = 1 * 10**6\n",
        "\n",
        "# opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "# init_epsilon = 1\n",
        "# final_epsilon = 0.1\n",
        "\n",
        "# loss_freq = 20\n",
        "# refresh_target_network_freq = 1000\n",
        "# eval_freq = 5000\n",
        "\n",
        "# max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-sD-QyUZdDA"
      },
      "outputs": [],
      "source": [
        "timesteps_per_epoch = 1\n",
        "batch_size = 32\n",
        "total_steps = 4 * 10**4\n",
        "decay_steps = 1 * 10**4\n",
        "\n",
        "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
        "\n",
        "init_epsilon = 1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "loss_freq = 20\n",
        "refresh_target_network_freq = 100\n",
        "eval_freq = 1000\n",
        "\n",
        "max_grad_norm = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piqDfKQAZdDA"
      },
      "outputs": [],
      "source": [
        "mean_rw_history = []\n",
        "td_loss_history = []\n",
        "grad_norm_history = []\n",
        "initial_state_v_history = []\n",
        "step = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks8NAV8AZdDA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def wait_for_keyboard_interrupt():\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU3GSGZqZdDA"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "with trange(step, total_steps + 1) as progress_bar:\n",
        "    for step in progress_bar:\n",
        "        if not utils.is_enough_ram():\n",
        "            print('less that 100 Mb RAM available, freezing')\n",
        "            print('make sure everything is ok and use KeyboardInterrupt to continue')\n",
        "            wait_for_keyboard_interrupt()\n",
        "\n",
        "        agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
        "\n",
        "        # play\n",
        "        _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
        "\n",
        "        # train\n",
        "        <YOUR CODE: sample batch_size of data from experience replay>\n",
        "\n",
        "        loss = <YOUR CODE: compute TD loss>\n",
        "\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if step % loss_freq == 0:\n",
        "            td_loss_history.append(loss.data.cpu().item())\n",
        "            grad_norm_history.append(grad_norm)\n",
        "\n",
        "        if step % refresh_target_network_freq == 0:\n",
        "            # Load agent weights into target_network\n",
        "            <YOUR CODE>\n",
        "\n",
        "        if step % eval_freq == 0:\n",
        "            mean_rw_history.append(evaluate(\n",
        "                make_env(), agent, n_games=3, greedy=True, t_max=1000, seed=step)\n",
        "            )\n",
        "            initial_state_q_values = agent.get_qvalues(\n",
        "                [make_env().reset(seed=step)[0]]\n",
        "            )\n",
        "            initial_state_v_history.append(np.max(initial_state_q_values))\n",
        "\n",
        "            clear_output(True)\n",
        "            print(\"buffer size = %i, epsilon = %.5f\" %\n",
        "                (len(exp_replay), agent.epsilon))\n",
        "\n",
        "            plt.figure(figsize=[16, 9])\n",
        "\n",
        "            plt.subplot(2, 2, 1)\n",
        "            plt.title(\"Mean reward per episode\")\n",
        "            plt.plot(mean_rw_history)\n",
        "            plt.grid()\n",
        "\n",
        "            assert not np.isnan(td_loss_history[-1])\n",
        "            plt.subplot(2, 2, 2)\n",
        "            plt.title(\"TD loss history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(td_loss_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 3)\n",
        "            plt.title(\"Initial state V\")\n",
        "            plt.plot(initial_state_v_history)\n",
        "            plt.grid()\n",
        "\n",
        "            plt.subplot(2, 2, 4)\n",
        "            plt.title(\"Grad norm history (smoothened)\")\n",
        "            plt.plot(utils.smoothen(grad_norm_history))\n",
        "            plt.grid()\n",
        "\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwWFT2SBZdDA"
      },
      "outputs": [],
      "source": [
        "final_score = evaluate(\n",
        "  make_env(),\n",
        "  agent, n_games=30, greedy=True, t_max=1000\n",
        ")\n",
        "print('final score:', final_score)\n",
        "assert final_score > 300, 'not good enough for DQN'\n",
        "print('Well done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-feeX9YZdDA"
      },
      "source": [
        "**Agent's predicted V-values vs their Monte-Carlo estimates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjVuSIrPZdDA"
      },
      "outputs": [],
      "source": [
        "eval_env = make_env()\n",
        "record = utils.play_and_log_episode(eval_env, agent)\n",
        "print('total reward for life:', np.sum(record['rewards']))\n",
        "for key in record:\n",
        "    print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCacwLw6ZdDA"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "ax.scatter(record['v_mc'], record['v_agent'])\n",
        "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
        "       'black', linestyle='--', label='x=y')\n",
        "\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "ax.set_title('State Value Estimates')\n",
        "ax.set_xlabel('Monte-Carlo')\n",
        "ax.set_ylabel('Agent')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
